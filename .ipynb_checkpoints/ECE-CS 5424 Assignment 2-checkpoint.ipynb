{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE-5424 / CS-5824 Advanced Machine Learning\n",
    "# Assignment 2\n",
    "In this homework, you will be using support vector machines (SVMs) to build a spam classifier. \n",
    "We will use the following packages/libraries:\n",
    "- [`numpy`](http://www.numpy.org/)\n",
    "- [`scipy`](https://docs.scipy.org/doc/scipy/reference/)\n",
    "- [`matplotlib`](https://matplotlib.org/)\n",
    "- [`scikit-learn`](https://scikit-learn.org/)\n",
    "\n",
    "\n",
    "To install them, you can use the following command in your `virtual environment`:\n",
    "- `pip install scipy`\n",
    "- `pip install scikit-learn`\n",
    "\n",
    "**You need to complete the following three sections**:\n",
    "1. Linear SVM\n",
    "2. Kernel SVM\n",
    "3. Spam classifier.\n",
    "\n",
    "All the materials here in the homework are modified from [Stanford CS229](http://cs229.stanford.edu/) and [Andrew Ng's Machine Learning course on Coursera](https://www.coursera.org/learn/machine-learning)\n",
    "\n",
    "\n",
    "## Submission guideline\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your Virginia Tech PID below.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells).\n",
    "4. Select Cell -> Run All. This will run all the cells in order.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. \n",
    "7. Zip BOTH the PDF file and this notebook. Rem\n",
    "8. Submit your zipped file .\n",
    "\n",
    "### Please Write Your VT PID Here: \n",
    "Your student ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required modules here.\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# We ignore the convergence warnings in this homework, as some of the exercise will\n",
    "# always trigger this warning.\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Enable auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Linear SVM [30 pts]\n",
    "\n",
    "In the first half of this exercise, you will be using support vector machines (SVMs) with various example 2D datasets. Experimenting with these datasets will help you gain an intuition of how SVMs work and how to use a Gaussian kernel with SVMs. In the next half of the exercise, you will be using support\n",
    "vector machines to build a spam classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Example Dataset 1 \n",
    "\n",
    "We will begin by with a 2D example dataset which can be separated by a linear boundary. The following cell plots the training data, which should look like this:\n",
    "\n",
    "![Dataset 1 training data](Figures/dataset1.png)\n",
    "\n",
    "In this dataset, the positions of the positive examples (indicated with `x`) and the negative examples (indicated with `o`) suggest a natural separation indicated by the gap. However, notice that there is an outlier positive example `x` on the far left at about (0.1, 4.1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, you will try using different values of the $C$ parameter with SVMs. Informally, the $C$ parameter is a positive value that controls the penalty for misclassified training examples. A large $C$ parameter tells the SVM to try to classify all the examples correctly. $C$ plays a role similar to $1/\\lambda$, where $\\lambda$ is the regularization parameter that we were using previously for logistic regression.\n",
    "\n",
    "\n",
    "We show the example of using $C=1$ and $C=100$ for Linear SVMs below:\n",
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"2\" style=\"text-align:center\">SVM Decision boundary for example dataset 1 </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">C=1<img src=\"Figures/svm_c1.png\"/></td>\n",
    "        <td style=\"text-align:center\">C=100<img src=\"Figures/svm_c100.png\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Your task here is to train Linear SVM classifier with [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) from `scikit-learn`.\n",
    "\n",
    "Specifically, you need to traing with `hinge_loss` and `l2` penalty, **try 5 different $C$**, and plot them out.\n",
    "\n",
    "**Hint:** check the class [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) (click it!) the function `fit` and `predict`, as well as relevant descriptions about **Parameters/Attributes**. Also, some of the arguments have its own default value (For example, the default $C$ for `LinearSVC` is 1.0). You can find all these information in the document.\n",
    "\n",
    "<br/>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** In this assignment, you DO NOT NEED TO add the extra feature $x_0 = 1$ for bias, as scikit-learn will automatically take care of it for you. So when passing your training data to the LinearSVC, there is no need to add this extra feature $x_0 = 1$ yourself. In particular, in python your code should be working with training examples $x \\in \\mathcal{R}^n$ (rather than $x \\in \\mathcal{R}^{n+1}$); for example, in the first example dataset $x \\in \\mathcal{R}^2$.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, grid=False):\n",
    "    \"\"\"\n",
    "    Plots the data points X and y into a new figure. Uses `+` for positive examples, and `o` for\n",
    "    negative examples. `X` is assumed to be a Mx2 matrix\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy ndarray\n",
    "        X is assumed to be a Mx2 matrix.\n",
    " \n",
    "    y : numpy ndarray\n",
    "        The data labels.\n",
    " \n",
    "    grid : bool (Optional)\n",
    "        Specify whether or not to show the grid in the plot. It is False by default.\n",
    " \n",
    "    Notes\n",
    "    -----\n",
    "    This was slightly modified such that it expects y=1 or y=0.\n",
    "    \"\"\"\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    # mew: marker edge width\n",
    "    # mec: marker edge color\n",
    "    # ms : marker size\n",
    "    # mfc: marker face color\n",
    "    plt.plot(X[pos, 0], X[pos, 1], 'X', mew=1, ms=10, mec='k')\n",
    "    plt.plot(X[neg, 0], X[neg, 1], 'o', mew=1, mfc='y', ms=10, mec='k')\n",
    "    \n",
    "def plot_linear_boundary(X, y, model):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary for linear SVM.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy ndarray\n",
    "        X is assumed to be a Mx2 matrix.\n",
    " \n",
    "    y : numpy ndarray\n",
    "        The data labels.\n",
    " \n",
    "    model : LinearSVC\n",
    "        Your trained SVM classifier.\n",
    "    \"\"\"   \n",
    "    w = model.coef_[0]    # The theta of your SVM classifier\n",
    "    b = model.intercept_  # The bias of your SVM classifier\n",
    "    xp = np.array([np.min(X[:, 0]), np.max(X[:, 0])])\n",
    "    yp = -(w[0] * xp + b) / w[1]\n",
    "    \n",
    "    plot_data(X, y)\n",
    "    plt.plot(xp, yp)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_nonlinear_boundary(X, y, model):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary for linear SVM.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy ndarray\n",
    "        X is assumed to be a Mx2 matrix.\n",
    " \n",
    "    y : numpy ndarray\n",
    "        The data labels.\n",
    " \n",
    "    model : SVC\n",
    "        Your trained SVM classifier.\n",
    "    \"\"\"   \n",
    "\n",
    "    x1 = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n",
    "    x2 = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    \n",
    "    vals = np.zeros(X1.shape)\n",
    "\n",
    "    for i in range(X1.shape[1]):\n",
    "        X_ = np.stack((X1[:, i], X2[:, i]), axis=1)\n",
    "        vals[:, i] = model.predict(X_)\n",
    "    \n",
    "    plt.contourf(X1, X2, vals, cmap='YlGnBu', alpha=0.2)    \n",
    "\n",
    "    \n",
    "    plot_data(X, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load from ex6data1\n",
    "# You will have X, y as keys in the dict data\n",
    "data = loadmat(os.path.join('Data', 'ex6data1.mat'))\n",
    "X, y = data['X'], data['y'][:, 0]\n",
    "\n",
    "# Plot training data\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try and plot with 5 different C. [15 pts]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 5 different C with LinearSVC.\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Pick 5 different C you like, train your LinearSVC with them, and plot all the#\n",
    "# decision boundaries.                                                         #\n",
    "# Note that you should train LinearSVC with l2 penalty and hinge loss.         #\n",
    "# Also, note that when passing arguments to functions/class initializer, you   #\n",
    "# can specify which value is for which argument.                               #\n",
    "# This trick is called keyword arguments in Python.                            #\n",
    "#                                                                              #\n",
    "# For example, if I want to make a LinearSVC with C=0.5 and squared hinge loss,#\n",
    "# we can write:                                                                #\n",
    "# LinearSVC(C=0.5, loss='squared_hinge')                                       #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question #1 [15 pts]:** Observing all the different decision boundaries, and answer the following two questions.\n",
    "- What is the difference between using difference $C$? Explain in two to three sentences.\n",
    "- What is the role of outlier in these plots? Describe your thought in two to three sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Kernel SVM [50 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "In this part of the exercise, you will be using SVMs to do non-linear classification. In particular, you will be using SVMs with Gaussian kernels on datasets that are not linearly separable.\n",
    "\n",
    "### 2.1 SVM with Gaussian kernel [30 pts]\n",
    "In the lectures, we have talked about how to find non-linear decision boundaries by using similarity $f_i$ as features for SVMs. In this section, your goal is to implement a Gaussian kernel for measuring the distance between a pair of examples ($x$, $l^{(i)}$):\n",
    "$$ f_{(i)}=\\text{similarity}\\left( x, l^{(i)} \\right) = \\exp \\left( - \\frac{\\left\\lvert\\left\\lvert x - l^{(i)} \\right\\lvert\\right\\lvert^2}{2\\sigma^2} \\right)$$\n",
    "Where we pick $l^{(1)}=x^{(1)}$, $l^{(2)}=x^{(2)}$, $\\cdots$, $l^{(m)}=x^{(m)}$, as mentioned in lecture slides. The Gaussian kernel is also parameterized by a bandwidth parameter, $\\sigma$, which determines how fast the similarity metric decreases (to 0) as the examples are further apart.\n",
    "\n",
    "Now, Complete the `gaussian_kernel` to compute the Gaussian kernel between two examples, ($x$, $l^{(i)}$).\n",
    "\n",
    "<a id=\"gaussianKernel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, l, sigma=2.0):\n",
    "    \"\"\"\n",
    "    Computes the radial basis function\n",
    "    Returns a radial basis function kernel between x and l.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x :  numpy ndarray\n",
    "        A matrix of size (m, n), representing the dataset.\n",
    "    \n",
    "    l : numpy ndarray\n",
    "        A matrix of size (k, n), representing the landmarks.\n",
    "    \n",
    "    sigma : float\n",
    "        The bandwidth parameter for the Gaussian kernel.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f : numpy ndarray\n",
    "        A matrix of size (m, k). Element f[i, j] represent the distance \n",
    "        between x[i] and l[j].\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return the similarity between `x` and `l`\n",
    "    computed using a Gaussian kernel with bandwidth `sigma`.\n",
    "    \"\"\"\n",
    "    f = 0\n",
    "\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement your gaussian kernel.                                              #\n",
    "    ################################################################################\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the function `gaussian_kernel` the following cell will test your kernel function on two provided examples and you should expect to see a value of:\n",
    "\n",
    "`[[1.         0.32465247]\n",
    " [0.32465247 1.        ]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[1, 2, 1], [0, 4, -1]])\n",
    "x2 = np.array([[1, 2, 1], [0, 4, -1]])\n",
    "sigma = 2\n",
    "\n",
    "sim = gaussian_kernel(x1, x2, sigma)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test it on the non-linear dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from ex6data2\n",
    "# You will have X, y as keys in the dict data\n",
    "data = loadmat(os.path.join('Data', 'ex6data2.mat'))\n",
    "X, y = data['X'], data['y'][:, 0]\n",
    "\n",
    "# Plot training data\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task here is to train a SVM classifier with [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from `scikit-learn`.\n",
    "\n",
    "While `SVC` has already implemented different kind of kernel functions, in this section, you need to use the `gaussian_kernel` that you just implemented, with `hinge_loss` and `l2` penalty.\n",
    "\n",
    "**Hint:** check the class [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (click it!) the function `fit` and `predict`, as well as relevant descriptions about **Parameters/Attributes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a decision boundary as shown in the figure below, as computed by the SVM with a Gaussian kernel. The decision boundary is able to separate most of the positive and negative examples correctly and follows the contours of the dataset well.\n",
    "\n",
    "![Dataset 2 decision boundary](Figures/svm_dataset2.png)\n",
    "\n",
    "<br/>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** While `SVC` allows you to pass your customized kernel function, they did not provide a way for you to specify $\\sigma$. Therefore, we have implemented a wrapper function called `kernel_wrapper` to help you circumvent this issue. The same thing can also be achieve by using functools.partial (See https://docs.python.org/2/library/functools.html#functools.partial).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def kernel_wrapper(kernel_func, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel_func :  function\n",
    "        Your gaussian kernel.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f : function\n",
    "        Your kernel function with your desired sigma.\n",
    "    \"\"\"\n",
    "    def f(x, l):\n",
    "        return kernel_func(x, l, sigma)\n",
    "    return f\n",
    "\n",
    "\n",
    "# sklearn does not let you pass sigma into your kernel function. \n",
    "# so to specify which sigma to use when computing the similarity,\n",
    "# we use the kernel_wrapper to help \"pack\" the sigma into your kernel function.\n",
    "\n",
    "sigma = 0.1 \n",
    "kernel_function = kernel_wrapper(gaussian_kernel, sigma=sigma)\n",
    "\n",
    "model = SVC(C=1.0, kernel=kernel_function, random_state=5566)\n",
    "model.fit(X, y)\n",
    "plot_nonlinear_boundary(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 2.2 Example Dataset 3 [20 pts]\n",
    "\n",
    "In this part of the homework, you will gain more practical skills on how to use a SVM with a Gaussian kernel. The next cell will load and display a third dataset, which should look like the figure below.\n",
    "\n",
    "![Dataset 3](Figures/dataset3.png)\n",
    "\n",
    "You will be using the SVM with the Gaussian kernel with this dataset. In the provided dataset, `ex6data3.mat`, you are given the variables `X`, `y`, `X_val`, `y_val`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from ex6data3\n",
    "# You will have X, y, Xval, yval as keys in the dict data\n",
    "data = loadmat(os.path.join('Data', 'ex6data3.mat'))\n",
    "\n",
    "X_train = data['X']\n",
    "y_train = data['y'][:, 0]\n",
    "X_val = data['Xval']\n",
    "y_val = data['yval'][:, 0]\n",
    "\n",
    "# Plot training data\n",
    "plot_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use the cross validation set `Xval`, `yval` to determine the best $C$ and $\\sigma$ parameter to use. You should write any additional code necessary to help you search over the parameters $C$ and $\\sigma$. For both $C$ and $\\sigma$, we suggest trying values in multiplicative steps (e.g., 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30).\n",
    "Note that you should try all possible pairs of values for $C$ and $\\sigma$ (e.g., $C = 0.3$ and $\\sigma = 0.1$). For example, if you try each of the 8 values listed above for $C$ and for $\\sigma^2$, you would end up training and evaluating (on the cross validation set) a total of $8^2 = 64$ different models. After you have determined the best $C$ and $\\sigma$ parameters to use, you should fill in the best parameters you found. For our best parameters, the SVM returned a decision boundary shown in the figure below. \n",
    "\n",
    "**Note that the best parameters may not be unique. You might get the same accuracy with a different decision boundary.**\n",
    "\n",
    "![](Figures/svm_dataset3_best.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)\n",
    "\n",
    "def search_hyperparam(X_train, y_train, X_val, y_val, Cs, sigmas):\n",
    "    \"\"\"\n",
    "    Returns your choice of C and sigma for Part 3 of the exercise \n",
    "    where you select the optimal (C, sigma) learning parameters to use for SVM\n",
    "    with RBF kernel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tran : array_like\n",
    "        (m x n) matrix of training data where m is number of training examples, and \n",
    "        n is the number of features.\n",
    "    \n",
    "    y_train : array_like\n",
    "        (m, ) vector of labels for ther training data.\n",
    "    \n",
    "    X_val : array_like\n",
    "        (mv x n) matrix of validation data where mv is the number of validation examples\n",
    "        and n is the number of features\n",
    "    \n",
    "    y_val : array_like\n",
    "        (mv, ) vector of labels for the validation data.\n",
    "    Cs    : array_like\n",
    "        list of C you wish to try.\n",
    "    sigmas: array_like\n",
    "        list of sigma you wish to try.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_C, best_sigma, best_accuracy : float, float, float\n",
    "        The best performing values for the regularization parameter C and \n",
    "        RBF parameter sigma.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    for C in Cs:\n",
    "        for sigma in sigmas:\n",
    "            kernel_function = kernel_wrapper(gaussian_kernel, sigma=sigma)\n",
    "            ################################################################################\n",
    "            # TODO:                                                                        #\n",
    "            # Perform cross validation to find the best value of C and sigma.              #\n",
    "            ################################################################################\n",
    "\n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ################################################################################    \n",
    "    return best_C, best_sigma, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code in the next cell trains the SVM classifier using the training set $(X, y)$ using parameters loaded from `dataset3Params`. Note that this might take a few minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try different SVM Parameters here\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Implement search_hyperparam to search over different combinations of C and   #\n",
    "# sigma and print out the best C, best sigma, and best accuracy.               #\n",
    "# You should be able to get an accuracy higher than 0.9. Note that the best    #\n",
    "# parameters are not unique.                                                   #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "# Train the model again with your best parameters.\n",
    "kernel_function = kernel_wrapper(gaussian_kernel, sigma=best_sigma)\n",
    "model = SVC(C=best_C, kernel=kernel_function)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plot_nonlinear_boundary(X_train, y_train, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## Section 3. Spam Classification [20 pts]\n",
    "\n",
    "Many email services today provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. In this part of the exercise, you will use SVMs to build your own spam filter.\n",
    "\n",
    "You will be training a classifier to classify whether a given email, $x$, is spam ($y = 1$) or non-spam ($y = 0$). In particular, you need to convert each email into a feature vector $x \\in \\mathbb{R}^n$ . The following parts of the exercise will walk you through how such a feature vector can be constructed from an email.\n",
    "\n",
    "The dataset included for this exercise is based on a a subset of the [SpamAssassin Public Corpus](http://spamassassin.apache.org/old/publiccorpus/). For the purpose of this exercise, you will only be using the body of the email (excluding the email headers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preprocessing Emails [10 pts]\n",
    "\n",
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset. The figure below shows a sample email that contains a URL, an email address (at the end), numbers, and dollar\n",
    "amounts.\n",
    "\n",
    "<img src=\"Figures/email.png\" width=\"700px\" />\n",
    "\n",
    "While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every\n",
    "email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the\n",
    "email with the unique string “httpaddr” to indicate that a URL was present.\n",
    "\n",
    "This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small. \n",
    "\n",
    "In the function `process_email` below, we have implemented the following email preprocessing and normalization steps:\n",
    "\n",
    "- **Lower-casing**: The entire email is converted into lower case, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "\n",
    "- **Stripping HTML**: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "\n",
    "- **Normalizing URLs**: All URLs are replaced with the text “httpaddr”.\n",
    "\n",
    "- **Normalizing Email Addresses**:  All email addresses are replaced with the text “emailaddr”.\n",
    "\n",
    "- **Normalizing Numbers**: All numbers are replaced with the text “number”.\n",
    "\n",
    "- **Normalizing Dollars**: All dollar signs ($) are replaced with the text “dollar”.\n",
    "\n",
    "- **Word Stemming**: Words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips off additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "\n",
    "- **Removal of non-words**: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n",
    "\n",
    "The result of these preprocessing steps is shown in the figure below. \n",
    "\n",
    "<img src=\"Figures/email_cleaned.png\" alt=\"email cleaned\" style=\"width: 600px;\"/>\n",
    "\n",
    "While preprocessing has left word fragments and non-words, this form turns out to be much easier to work with for performing feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Vocabulary List\n",
    "\n",
    "After preprocessing the emails, we have a list of words for each email. The next step is to choose which words we would like to use in our classifier and which we would want to leave out.\n",
    "\n",
    "For this exercise, we have chosen only the most frequently occuring words as our set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the\n",
    "model to overfit our training set. The complete vocabulary list is in the file `vocab.txt` (inside the `Data` directory for this exercise) and also shown in the figure below.\n",
    "\n",
    "<img src=\"Figures/vocab.png\" alt=\"Vocab\" width=\"150px\" />\n",
    "\n",
    "Our vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus,\n",
    "resulting in a list of 1899 words. In practice, a vocabulary list with about 10,000 to 50,000 words is often used.\n",
    "Given the vocabulary list, we can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary dictionary. The figure below shows the mapping for the sample email. Specifically, in the sample email, the word “anyone” was first normalized to “anyon” and then mapped onto the index 86 in the vocabulary list.\n",
    "\n",
    "<img src=\"Figures/word_indices.png\" alt=\"word indices\" width=\"200px\" />\n",
    "\n",
    "Your task now is to complete the code in the function `processEmail` to perform this mapping. In the code, you are given a string `word` which is a single word from the processed email. You should look up the word in the vocabulary list `vocabList`. If the word exists in the list, you should add the index of the word into the `word_indices` variable. If the word does not exist, and is therefore not in the vocabulary, you can skip the word.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**python tip**: In python, you can find the index of the first occurence of an item in `list` using the  `index` attribute. In the provided code for `processEmail`, `vocabList` is a python list containing the words in the vocabulary. To find the index of a word, we can use `vocabList.index(word)` which would return a number indicating the index of the word within the list. If the word does not exist in the list, a `ValueError` exception is raised. In python, we can use the `try/except` statement to catch exceptions which we do not want to stop the program from running. You can think of the `try/except` statement to be the same as an `if/else` statement, but it asks for forgiveness rather than permission.\n",
    "\n",
    "An example would be:\n",
    "<br>\n",
    "\n",
    "```\n",
    "try:\n",
    "    do stuff here\n",
    "except ValueError:\n",
    "    pass\n",
    "    # do nothing (forgive me) if a ValueError exception occured within the try statement\n",
    "```\n",
    "</div>\n",
    "<a id=\"processEmail\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email(email_contents, verbose=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the body of an email and returns a list of indices \n",
    "    of the words contained in the email.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    email_contents : str\n",
    "        A string containing one email. \n",
    "    \n",
    "    verbose : bool\n",
    "        If True, print the resulting email after processing.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word_indices : list\n",
    "        A list of integers containing the index of each word in the \n",
    "        email which is also present in the vocabulary.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to add the index of word to word_indices \n",
    "    if it is in the vocabulary. At this point of the code, you have \n",
    "    a stemmed word from the email in the variable word.\n",
    "    You should look up word in the vocabulary list (vocabList). \n",
    "    If a match exists, you should add the index of the word to the word_indices\n",
    "    list. Concretely, if word = 'action', then you should\n",
    "    look up the vocabulary list to find where in vocabList\n",
    "    'action' appears. For example, if vocabList[18] =\n",
    "    'action', then, you should add 18 to the word_indices \n",
    "    vector (e.g., word_indices.append(18)).\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - vocabList[idx] returns a the word with index idx in the vocabulary list.\n",
    "    \n",
    "    - vocabList.index(word) return index of word `word` in the vocabulary list.\n",
    "      (A ValueError exception is raised if the word does not exist.)\n",
    "    \"\"\"\n",
    "    # Load Vocabulary\n",
    "    vocabList = utils.getVocabList()\n",
    "\n",
    "    # Init return value\n",
    "    word_indices = []\n",
    "\n",
    "    # ========================== Preprocess Email ===========================\n",
    "    # Find the Headers ( \\n\\n and remove )\n",
    "    # Uncomment the following lines if you are working with raw emails with the\n",
    "    # full headers\n",
    "    # hdrstart = email_contents.find(chr(10) + chr(10))\n",
    "    # email_contents = email_contents[hdrstart:]\n",
    "\n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    # Strip all HTML\n",
    "    # Looks for any expression that starts with < and ends with > and replace\n",
    "    # and does not have any < or > in the tag it with a space\n",
    "    email_contents =re.compile('<[^<>]+>').sub(' ', email_contents)\n",
    "\n",
    "    # Handle Numbers\n",
    "    # Look for one or more characters between 0-9\n",
    "    email_contents = re.compile('[0-9]+').sub(' number ', email_contents)\n",
    "\n",
    "    # Handle URLS\n",
    "    # Look for strings starting with http:// or https://\n",
    "    email_contents = re.compile('(http|https)://[^\\s]*').sub(' httpaddr ', email_contents)\n",
    "\n",
    "    # Handle Email Addresses\n",
    "    # Look for strings with @ in the middle\n",
    "    email_contents = re.compile('[^\\s]+@[^\\s]+').sub(' emailaddr ', email_contents)\n",
    "    \n",
    "    # Handle $ sign\n",
    "    email_contents = re.compile('[$]+').sub(' dollar ', email_contents)\n",
    "    \n",
    "    # get rid of any punctuation\n",
    "    email_contents = re.split('[ @$/#.-:&*+=\\[\\]?!(){},''\">_<;%\\n\\r]', email_contents)\n",
    "\n",
    "    # remove any empty word string\n",
    "    email_contents = [word for word in email_contents if len(word) > 0]\n",
    "    \n",
    "    # Stem the email contents word by word\n",
    "    stemmer = utils.PorterStemmer()\n",
    "    processed_email = []\n",
    "    \n",
    "    for word in email_contents:\n",
    "        # Remove any remaining non alphanumeric characters in word\n",
    "        word = re.compile('[^a-zA-Z0-9]').sub('', word).strip()\n",
    "        word = stemmer.stem(word)\n",
    "        processed_email.append(word)\n",
    "\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Look up the word in the dictionary and add to word_indices if found.         # \n",
    "        ################################################################################\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "    if verbose:\n",
    "        print('----------------')\n",
    "        print('Processed email:')\n",
    "        print('----------------')\n",
    "        print(' '.join(processed_email))\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented `processEmail`, the following cell will run your code on the email sample and you should see an output of the processed email and the indices list mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To use an SVM to classify emails into Spam v.s. Non-Spam, you first need\n",
    "#  to convert each email into a vector of features. In this part, you will\n",
    "#  implement the preprocessing steps for each email. You should\n",
    "#  complete the code in processEmail.m to produce a word indices vector\n",
    "#  for a given email.\n",
    "\n",
    "# Extract Features\n",
    "with open(os.path.join('Data', 'emailSample1.txt')) as fid:\n",
    "    file_contents = fid.read()\n",
    "\n",
    "word_indices  = process_email(file_contents)\n",
    "\n",
    "#Print Stats\n",
    "print('-------------')\n",
    "print('Word Indices:')\n",
    "print('-------------')\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 3.2 Extracting Features from Emails [10 pts]\n",
    "\n",
    "You will now implement the feature extraction that converts each email into a vector in $\\mathbb{R}^n$. For this exercise, you will be using n = # words in vocabulary list. Specifically, the feature $x_i \\in \\{0, 1\\}$ for an email corresponds to whether the $i^{th}$ word in the dictionary occurs in the email. That is, $x_i = 1$ if the $i^{th}$ word is in the email and $x_i = 0$ if the $i^{th}$ word is not present in the email.\n",
    "\n",
    "Thus, for a typical email, this feature would look like:\n",
    "\n",
    "$$ x = \\begin{bmatrix} \n",
    "0 & \\dots & 1 & 0 & \\dots & 1 & 0 & \\dots & 0 \n",
    "\\end{bmatrix}^T \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "You should now complete the code in the function `email_features` to generate a feature vector for an email, given the `word_indices`.\n",
    "<a id=\"emailFeatures\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_features(word_indices):\n",
    "    \"\"\"\n",
    "    Takes in a word_indices vector and produces a feature vector from the word indices. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word_indices : list\n",
    "        A list of word indices from the vocabulary list.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : list \n",
    "        The computed feature vector.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return a feature vector for the\n",
    "    given email (word_indices). To help make it easier to  process \n",
    "    the emails, we have have already pre-processed each email and converted\n",
    "    each word in the email into an index in a fixed dictionary (of 1899 words).\n",
    "    The variable `word_indices` contains the list of indices of the words \n",
    "    which occur in one email.\n",
    "    \n",
    "    Concretely, if an email has the text:\n",
    "\n",
    "        The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "    Then, the word_indices vector for this text might look  like:\n",
    "               \n",
    "        60  100   33   44   10     53  60  58   5\n",
    "\n",
    "    where, we have mapped each word onto a number, for example:\n",
    "\n",
    "        the   -- 60\n",
    "        quick -- 100\n",
    "        ...\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The above numbers are just an example and are not the actual mappings.\n",
    "\n",
    "    Your task is take one such `word_indices` vector and construct\n",
    "    a binary feature vector that indicates whether a particular\n",
    "    word occurs in the email. That is, x[i] = 1 when word i\n",
    "    is present in the email. Concretely, if the word 'the' (say,\n",
    "    index 60) appears in the email, then x[60] = 1. The feature\n",
    "    vector should look like:\n",
    "        x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..]\n",
    "    \"\"\"\n",
    "    # Total number of words in the dictionary\n",
    "    n = 1899\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    x = np.zeros(n)\n",
    "\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Set the corresponding word indices to 1.                                     #\n",
    "    ################################################################################\n",
    "\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented `emailFeatures`, the next cell will run your code on the email sample. You should see that the feature vector had length 1899 and 45 non-zero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features\n",
    "with open(os.path.join('Data', 'emailSample1.txt')) as fid:\n",
    "    file_contents = fid.read()\n",
    "\n",
    "word_indices  = process_email(file_contents)\n",
    "features      = email_features(word_indices)\n",
    "\n",
    "# Print Stats\n",
    "print('\\nLength of feature vector: %d' % len(features))\n",
    "print('Number of non-zero entries: %d' % sum(features > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training SVM for Spam Classification\n",
    "\n",
    "In the following section we will load a preprocessed training dataset that will be used to train a SVM classifier. The file `spamTrain.mat` (within the `Data` folder for this exercise) contains 4000 training examples of spam and non-spam email, while `spamTest.mat` contains 1000 test examples. Each\n",
    "original email was processed using the `process_email` and `email_features` functions and converted into a vector $x^{(i)} \\in \\mathbb{R}^{1899}$.\n",
    "\n",
    "After loading the dataset, the next cell proceed to train a `LinearSVC` to classify between spam ($y = 1$) and non-spam ($y = 0$) emails. Once the training completes, you should see that the classifier gets a training accuracy of about 99.8% and a test accuracy of about 98.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spam Email dataset\n",
    "# You will have X, y in your environment\n",
    "data = loadmat(os.path.join('Data', 'spamTrain.mat'))\n",
    "X, y= data['X'].astype(float), data['y'][:, 0]\n",
    "\n",
    "print('Training Linear SVM (Spam Classification)')\n",
    "\n",
    "C = 0.1\n",
    "model = LinearSVC(C=C, penalty='l2', loss='hinge', random_state=5566)\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# You will have Xtest, ytest in your environment\n",
    "data = loadmat(os.path.join('Data', 'spamTest.mat'))\n",
    "Xtest, ytest = data['Xtest'].astype(float), data['ytest'][:, 0]\n",
    "\n",
    "print('Evaluating the trained Linear SVM on a test set ...')\n",
    "p = model.predict(Xtest)\n",
    "\n",
    "print('Test Accuracy: %.2f' % (np.mean(p == ytest) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Top Predictors for Spam\n",
    "\n",
    "To better understand how the spam classifier works, we can inspect the parameters to see which words the classifier thinks are the most predictive of spam. The next cell finds the parameters with the largest positive values in the classifier and displays the corresponding words similar to the ones shown in the figure below.\n",
    "\n",
    "<div style=\"border-style: solid; border-width: 1px; margin: 10px 10px 10px 10px; padding: 10px 10px 10px 10px\">\n",
    "our  click  remov guarante visit basenumb dollar pleas price will nbsp most lo ga hour\n",
    "</div>\n",
    "\n",
    "Thus, if an email contains words such as “guarantee”, “remove”, “dollar”, and “price” (the top predictors shown in the figure), it is likely to be classified as spam.\n",
    "\n",
    "Since the model we are training is a linear SVM, we can inspect the weights learned by the model to understand better how it is determining whether an email is spam or not. The following code finds the words with the highest weights in the classifier. Informally, the classifier 'thinks' that these words are the most likely indicators of spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the weights and obtin the vocabulary list\n",
    "# NOTE some words have the same weights, \n",
    "# so their order might be different than in the text above\n",
    "weights = model.coef_[0]\n",
    "\n",
    "idx = np.argsort(weights)\n",
    "top_idx = idx[-15:][::-1]\n",
    "vocabList = utils.getVocabList()\n",
    "\n",
    "print('Top predictors of spam:')\n",
    "print('%-15s %-15s' % ('word', 'weight'))\n",
    "print('----' + ' '*12 + '------')\n",
    "for word, w in zip(np.array(vocabList)[top_idx], weights[top_idx]):\n",
    "    print('%-15s %0.2f' % (word, w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Try your own emails (Optional)\n",
    "\n",
    "Now that you have trained a spam classifier, you can start trying it out on your own emails. In the starter code, we have included two email examples (`emailSample1.txt` and `emailSample2.txt`) and two spam examples (`spamSample1.txt` and `spamSample2.txt`). The next cell runs the spam classifier over the first spam example and classifies it using the learned SVM. You should now try the other examples we have provided and see if the classifier gets them right. You can also try your own emails by replacing the examples (plain text files) with your own emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('Data', 'emailSample1.txt')\n",
    "\n",
    "with open(filename) as fid:\n",
    "    file_contents = fid.read()\n",
    "\n",
    "word_indices = process_email(file_contents, verbose=False)\n",
    "x = email_features(word_indices)\n",
    "print(x.shape)\n",
    "p = model.predict(x.reshape(1, -1))\n",
    "\n",
    "print('\\nProcessed %s\\nSpam Classification: %s' % (filename, 'spam' if p else 'not spam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
